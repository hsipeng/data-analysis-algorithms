{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树——是一种被广泛使用的分类算法。相比贝叶斯算法，决策树的优势在于构造过程不需要任何领域知识或参数设置。在实际应用中，对于探测式的知识发现，决策树更加适用。\n",
    "\n",
    "# 决策树通常有三个步骤：特征选择、决策树的生成、决策树的修剪。\n",
    "\n",
    "# 决策树分类算法的关键就是根据”先验数据“构造一棵最佳的决策树，用以预测未知数据的类别\n",
    "\n",
    "# 决策树：是一个树结构(可以是二叉树或非二叉树)。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。\n",
    "\n",
    "#  算法实现\n",
    "\n",
    "# 梳理出数据中的属性，比较按照某特定属性划分后的数据的信息熵增益，选择信息熵增益最大的那个属性作为第一划分依据，然后继续选择第二属性，以此类推。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2\n1\n{'声音': {'细': '女', '粗': {'头发': {'短': '男', '长': '女'}}}}\n"
    }
   ],
   "source": [
    "from math import log\n",
    "import operator\n",
    "\n",
    "def calcShannonEnt(dataSet):  # 计算数据的熵(entropy)\n",
    "    numEntries=len(dataSet)  # 数据条数\n",
    "    labelCounts={}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1] # 每行数据的最后一个字（类别）\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1  # 统计有多少个类以及每个类的数量\n",
    "    shannonEnt=0\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries # 计算单个类的熵值\n",
    "        shannonEnt-=prob*log(prob,2) # 累加每个类的熵值\n",
    "    return shannonEnt\n",
    "\n",
    "def createDataSet1(): # 创造示例数据\n",
    "    dataSet = [['长', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['长', '细', '女'],\n",
    "               ['短', '细', '女'],\n",
    "               ['短', '粗', '女'],\n",
    "               ['长', '粗', '女'],\n",
    "               ['长', '粗', '女']]\n",
    "    labels = ['头发', '声音'] # 两个特征\n",
    "    return dataSet, labels\n",
    "\n",
    "def splitDataSet(dataSet, axis, value): # 按某个特征分类后的数据\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet): # 选择最优的分类特征\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    print(numFeatures)\n",
    "    baseEntropy = calcShannonEnt(dataSet) # 原始的熵\n",
    "    bestInfoGain = 0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet) # 按特征分类后的熵\n",
    "        infoGain = baseEntropy - newEntropy # 原始熵与按特征分类后的熵的差值\n",
    "        if (infoGain > bestInfoGain): # 若按某特征划分后，熵值减少的最大，则次特征为最优分类特征\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "def majorityCnt(classList): # 按分类后类别数量排序，比如：最后分类为2男1女，则判定为男：\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def createTree(dataSet, labels):\n",
    "    classList = [example[-1] for example in dataSet] # 类别：男或女\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet) # 选择最优特征\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}} # 分类结果以字典形式保存\n",
    "    del(labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    #print(featValues)\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n",
    "    return myTree\n",
    "\n",
    "dataSet, labels = createDataSet1() # 创造示例数据\n",
    "print(createTree(dataSet, labels)) # 输出决策树模型结果\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}